{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a5a74d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook, we will test out pulling out a mistral-7b and serving with [LocalAI](https://localai.io/) in SAP AI Core. You can also run Llama 3, Phi 3, Mistral, Mixtral, Gemma, Llava and other [supported models in LocalAI](https://localai.io/models/). \n",
    "\n",
    "### Prerequisites\n",
    "Before running this notebook, please assure you have performed the [Prerequisites](../../README.md) and [01-deployment.ipynb](01-deployment.ipynb). As a result, a deployment of local-ai scenario is running in SAP AI Core. <br/><br/>\n",
    "\n",
    "If the configuration and deployment are created through SAP AI Launchpad, please manually update the configuration_id and deployment_id in [env.json](env.json)\n",
    "```json\n",
    "{\n",
    "    \"configuration_id\": \"<YOUR_CONFIGURATION_ID_OF_LOCALAI_SCENARIO>\",\n",
    "    \"deployment_id\": \"<YOUR_DEPLOYMENT_ID_BASED_ON_CONFIG_ABOVE>\"\n",
    "}\n",
    "```\n",
    " \n",
    "### The high-level flow:\n",
    "- Load configurations info\n",
    "- Connect to SAP AI Core via SDK\n",
    "- Check the status and logs of the deployment\n",
    "- Install model from LocalAI's model gallery through API\n",
    "- Inference the model with OpenAI-compatible chat completion API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55bd7b",
   "metadata": {},
   "source": [
    "#### 1.Load config info \n",
    "- resource_group loaded from [config.json](../config.json)\n",
    "- deployment_id(created in 01-deployment.ipynb) loaded [env.json](env.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90f1e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from ai_api_client_sdk.ai_api_v2_client import AIAPIV2Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5eee26b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment id:  d09391ae26f8c16d  resource group:  oss-llm\n"
     ]
    }
   ],
   "source": [
    "# Please replace the configurations below.\n",
    "# config_id: The target configuration to create the deployment. Please create the configuration first.\n",
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "with open(\"./env.json\") as f:\n",
    "    env = json.load(f)\n",
    "\n",
    "deployment_id = env[\"deployment_id\"]\n",
    "resource_group = config.get(\"resource_group\", \"default\")\n",
    "print(\"deployment id: \", deployment_id, \" resource group: \", resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd694c3",
   "metadata": {},
   "source": [
    "#### 2.Initiate connection to SAP AI Core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a4cc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_sk = config[\"ai_core_service_key\"]\n",
    "base_url = aic_sk[\"serviceurls\"][\"AI_API_URL\"] + \"/v2/lm\"\n",
    "ai_api_client = AIAPIV2Client(\n",
    "    base_url= base_url,\n",
    "    auth_url=aic_sk[\"url\"] + \"/oauth/token\",\n",
    "    client_id=aic_sk['clientid'],\n",
    "    client_secret=aic_sk['clientsecret'],\n",
    "    resource_group=resource_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ffb297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = ai_api_client.rest_client.get_token()\n",
    "headers = {\n",
    "        \"Authorization\": token,\n",
    "        'ai-resource-group': resource_group,\n",
    "        \"Content-Type\": \"application/json\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7b416",
   "metadata": {},
   "source": [
    "#### 3.Check the deployment status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d46cf76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment-d09391ae26f8c16d is running. Ready for inference request\n"
     ]
    }
   ],
   "source": [
    "# Check deployment status before inference request\n",
    "deployment_url = f\"{base_url}/deployments/{deployment_id}\"\n",
    "response = requests.get(url=deployment_url, headers=headers)\n",
    "resp = response.json()    \n",
    "status = resp['status']\n",
    "\n",
    "deployment_log_url = f\"{base_url}/deployments/{deployment_id}/logs\"\n",
    "if status == \"RUNNING\":\n",
    "        print(f\"Deployment-{deployment_id} is running. Ready for inference request\")\n",
    "else:\n",
    "        print(f\"Deployment-{deployment_id} status: {status}. Not yet ready for inference request\")\n",
    "        #retrieve deployment logs\n",
    "        #{{apiurl}}/v2/lm/deployments/{{deploymentid}}/logs.\n",
    "\n",
    "        response = requests.get(deployment_log_url, headers=headers)\n",
    "        print('Deployment Logs:\\n', response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b4fcb",
   "metadata": {},
   "source": [
    "#### 4.Install the model\n",
    "We'll install the model mistral from LocalAI's [Model Gallery](https://localai.io/models/), in which GPU is not enabled by default. Then, we'll override the model configuration by enabling GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d86047d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral\"\n",
    "#model = \"mistral:7b-instruct-q5_K_M\"\n",
    "#model = \"mixtral:8x7b-instruct-v0.1-q4_0\" #Important: please resource plan to infer.l in byom-oss-llm-templates/local-ai-template.yaml\n",
    "deployment = ai_api_client.deployment.get(deployment_id)\n",
    "inference_base_url = f\"{deployment.deployment_url}/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f50641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d09391ae26f8c16d/v1/models/apply\n",
      "Result: {'uuid': '0293dd4d-eb6f-11ee-ba84-463fc5800695', 'status': 'http://d09391ae26f8c16d-predictor-default.rg-2ac5c88d-3284ef4c.svc.cluster.local/models/jobs/0293dd4d-eb6f-11ee-ba84-463fc5800695'}\n"
     ]
    }
   ],
   "source": [
    "# Install model from model gallery. \n",
    "# Please refer to this for find out your target model: https://localai.io/models/#how-to-install-a-model-from-the-repositories\n",
    "install_model_endpoint = f\"{inference_base_url}/models/apply\"\n",
    "print(install_model_endpoint)\n",
    "\n",
    "json_data = {\n",
    "    # Installation with the id from model gallery is working well.\n",
    "    \"id\": \"model-gallery@mistral\"\n",
    "    #\"id\": \"model-gallery@bert-embeddings\"\n",
    "\n",
    "    # Installation with url isn't working as described here: https://localai.io/models/#how-to-install-a-model-without-a-gallery\n",
    "    #\"url\": \"github:go-skynet/model-gallery/blob/main/bert-embeddings.yaml\"\n",
    "    #\"url\": \"github:mudler/LocalAI/blob/master/examples/configurations/phi-2.yaml\"\n",
    "    #\"url\": \"github:YatseaLi/byom-oss-llm-ai-core/main/byom-oss-llm-code/local-ai/configurations/mistral.yaml\"\n",
    "  }\n",
    "response = requests.post(url=install_model_endpoint, headers=headers, json=json_data)\n",
    "install_model_resp_json = response.json()\n",
    "job_id = install_model_resp_json[\"uuid\"]\n",
    "print('Result:', install_model_resp_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ac9e5e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Installation Job Logs:\n",
      " {\"file_name\":\"\",\"error\":null,\"processed\":true,\"message\":\"completed\",\"progress\":100,\"file_size\":\"\",\"downloaded_size\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "#Checking status of installing model\n",
    "#deployment_log_url = f\"{base_url}/deployments/{deployment_id}/logs?start=2021-05-19T00:00:14.347Z&\"\n",
    "endpoint = f\"{inference_base_url}/models/jobs/{job_id}\"\n",
    "response = requests.get(endpoint, headers=headers )\n",
    "print('Model Installation Job Logs:\\n', response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e79ef9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d09391ae26f8c16d/v1/models/apply\n",
      "Result: {'uuid': '3711de74-eb6f-11ee-ba84-463fc5800695', 'status': 'http://d09391ae26f8c16d-predictor-default.rg-2ac5c88d-3284ef4c.svc.cluster.local/models/jobs/3711de74-eb6f-11ee-ba84-463fc5800695'}\n"
     ]
    }
   ],
   "source": [
    "# Install model from hugging face without model config. GPU is not enabled by default. \n",
    "# In this example, let's override the configuration with GPU enabled while downloading the model from hugging face.\n",
    "# Please refer to this for find out your target model: https://localai.io/models/#how-to-install-a-model-from-the-repositories\n",
    "install_model_endpoint = f\"{inference_base_url}/models/apply\"\n",
    "print(install_model_endpoint)\n",
    "\n",
    "json_data = {\n",
    "     \"id\": \"model-gallery@mistral\",\n",
    "     #\"id\": \"huggingface@TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
    "     \"name\": model, #rename the model\n",
    "     # Override with GPU enabled.\n",
    "     \"overrides\": { \n",
    "        \"f16\": True,\n",
    "        \"mmap\": True,\n",
    "        \"gpu_layers\": 33,\n",
    "        \"threads\": 3 \n",
    "      }\n",
    "   }\n",
    "\n",
    "response = requests.post(install_model_endpoint, headers=headers, json=json_data)\n",
    "install_model_resp_json = response.json()\n",
    "job_id = install_model_resp_json[\"uuid\"]\n",
    "print('Result:', install_model_resp_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3071b7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Installation Job Logs:\n",
      " {\"file_name\":\"\",\"error\":null,\"processed\":true,\"message\":\"completed\",\"progress\":100,\"file_size\":\"\",\"downloaded_size\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "#Checking status of installing model\n",
    "#deployment_log_url = f\"{base_url}/deployments/{deployment_id}/logs?start=2021-05-19T00:00:14.347Z&\"\n",
    "endpoint = f\"{inference_base_url}/models/jobs/{job_id}\"\n",
    "response = requests.get(url=endpoint, headers=headers )\n",
    "print('Model Installation Job Logs:\\n', response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "118cab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d09391ae26f8c16d/v1/v1/models\n",
      "Result: {\"object\":\"list\",\"data\":[{\"id\":\"mistral\",\"object\":\"model\"}]}\n"
     ]
    }
   ],
   "source": [
    "# List models and found out the model id, which will  be used in next request on chat completion \n",
    "endpoint = f\"{inference_base_url}/v1/models\"\n",
    "print(endpoint)\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7d13c",
   "metadata": {},
   "source": [
    "#### 5.Inference completion and chat completion APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0658246",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_chat_api_endpoint = f\"{inference_base_url}/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848db40",
   "metadata": {},
   "source": [
    "##### Sample#1: Test OpenAI compatible API for Chat Completion\n",
    "Now let's test its [OpenAI compatible API for Chat Completion](https://localai.io/features/text-generation/#chat-completions), which is the exact API interface of Chat Completion of GPT-3.5/4 in SAP Generative AI Hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d3725c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"created\":1711456731,\"object\":\"chat.completion\",\"id\":\"f9eecc4d-49e5-4c6f-86c7-12a9ca99b653\",\"model\":\"mistral\",\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"message\":{\"role\":\"assistant\",\"content\":\" The sky appears blue because of the way light scatters in the Earth's atmosphere. When sunlight enters the Earth's atmosphere, it consists of a range of colors, including red, orange, yellow, green, blue, indigo, and violet, which together form the visible light spectrum.\\n\\nAs the sunlight travels through the atmosphere, it interacts with the air molecules and tiny particles called aerosols and dust. These particles are much smaller than the wavelength of visible light. When the sunlight encounters these particles, it can be scattered in different directions.\\n\\nThe shorter wavelength colors, such as blue and violet, are scattered more efficiently than the longer wavelength colors, like red and orange. This is because the particles in the atmosphere are much smaller than the wavelength of the longer colors, so they can pass through without being scattered.\\n\\nAs a result, when we look up at the sky, we predominantly see the blue and violet colors that have been scattered by the atmosphere. The other colors are still present, but they are not as easily seen due to their reduced scattering.\\n\\nAdditionally, the blue light is scattered more effectively by the smaller particles in the atmosphere, which is why the sky appears more blue during the day when the Sun is higher in the sky and the sunlight has to travel through more of the atmosphere. At sunrise and sunset, the sunlight has to travel through less atmosphere, and the sky appears more reddish-orange due to the increased scattering of the longer wavelength colors.\\u003cdummy32000\\u003e\"}}],\"usage\":{\"prompt_tokens\":0,\"completion_tokens\":0,\"total_tokens\":0}}\n"
     ]
    }
   ],
   "source": [
    "#let's try its openai-compatible chat completion api\n",
    "sys_msg = \"You are an helpful AI assistant\"\n",
    "user_msg = \"why the sky is blue?\"\n",
    "\n",
    "json_data = { \n",
    "  \"model\": model, \n",
    "  \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                #\"content\": \"why is the sky blue?\"\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "response = requests.post(openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c9f16",
   "metadata": {},
   "source": [
    "##### Sample#2: Write a haiku about running LocalAI in AI Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c824efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'created': 1711456731, 'object': 'chat.completion', 'id': 'f9eecc4d-49e5-4c6f-86c7-12a9ca99b653', 'model': 'mistral', 'choices': [{'index': 0, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': ' Swiftly, the AI Core runs,\\n Processing data with grace,\\n LocalAI, a shining star.\\n'}}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "#let's test its openai-compatible chat completion api by writing a haiku\n",
    "sys_msg = \"You are a helpful assistant\"\n",
    "user_msg = \"Write a haiku for running LocalAI in AI Core\"\n",
    "json_data = {\n",
    "  \"model\": model,\n",
    "  \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "response = requests.post(openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b4ab1",
   "metadata": {},
   "source": [
    "##### Sample#3: Customer Message Processing \n",
    "In our sample [btp-industry-use-cases/04-customer-interaction-gpt4](https://github.com/SAP-samples/btp-industry-use-cases/tree/main/04-customer-interaction-gpt4),GPT-3.5/4 is used to process customer messages in customer interactions and output in json schema with plain prompting.\n",
    "- Summarize customer message into title and a short description\n",
    "- Analyze the sentiment of the customer message\n",
    "- Extract the entities from the customer message, such as customer, product, order no etc.\n",
    "\n",
    "Let's see if the same scenario could be achieved with mistral-7b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "74cfc90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"created\":1711456731,\"object\":\"chat.completion\",\"id\":\"f9eecc4d-49e5-4c6f-86c7-12a9ca99b653\",\"model\":\"mistral\",\"choices\":[{\"index\":0,\"finish_reason\":\"stop\",\"message\":{\"role\":\"assistant\",\"content\":\" {\\\"sentiment\\\":\\\"Negative\\\",\\\"title\\\":\\\"Coffee Machine Stopping Brewing After Short Use\\\",\\\"summary\\\":\\\"Customer reports coffee machine stopping brewing after a few seconds and milk frother not working properly. The machine was fixed under warranty but the issue reoccurred.\\\",\\\"entities\\\":[{\\\"field\\\":\\\"product_no\\\",\\\"value\\\":\\\"\\\"},{\\\"field\\\":\\\"product_name\\\",\\\"value\\\":\\\"Coffee Machine\\\"},{\\\"field\\\":\\\"customer_name\\\",\\\"value\\\":\\\"\\\"},{\\\"field\\\":\\\"store_name\\\",\\\"value\\\":\\\"Harvey Norman\\\"},{\\\"field\\\":\\\"order_no\\\",\\\"value\\\":\\\"\\\"},{\\\"field\\\":\\\"order_date\\\",\\\"value\\\":\\\"\\\"},{\\\"field\\\":\\\"delivery_no\\\",\\\"value\\\":\\\"\\\"},{\\\"field\\\":\\\"delivery_date\\\",\\\"value\\\":\\\"\\\"},{\\\"field\\\":\\\"invoice_no\\\",\\\"value\\\":\\\"\\\"},{\\\"field\\\":\\\"invoice_date\\\",\\\"value\\\":\\\"\\\"},{\\\"field\\\":\\\"product_price\\\",\\\"value\\\":\\\"$1,349\\\"}]}\\u003cdummy32000\\u003e\"}}],\"usage\":{\"prompt_tokens\":0,\"completion_tokens\":0,\"total_tokens\":0}}\n"
     ]
    }
   ],
   "source": [
    "# Let's test its openai-compatible chat completion api with to process customer message with\n",
    "# summarization, sentiment analysis and entities extraction and output as json\n",
    "sys_msg = r'''\n",
    "You are an AI assistant to process the input text. Here are your tasks on the text.\n",
    "1.Apply Sentiment Analysis\n",
    "2.Generate a title less than 100 characters,and summarize the text into a short description less than 200 characters\n",
    "3.Extract the entities such as customer,product,order,delivery,invoice etc from the text Here is a preliminary list of the target entity fields and description. Please extract all the identifiable entities even not in the list below. Don't include any field with unknown value. \\\n",
    "-customer_no: alias customer number, customer id, account id, account number which could be used to identify a customer.\n",
    "-customer_name: customer name, account name\n",
    "-customer_phone: customer contact number. -product_no: product number, product id\n",
    "-product_name\n",
    "-order_no: sales order number, order id\n",
    "-order_date \n",
    "-delivery_no: delivery number, delivery id\n",
    "-delivery_date: delivery date, shipping date\n",
    "-invoice_no: alias invoice number, invoice id, receipt number, receipt id etc. which can be used to locate a invoice.\n",
    "-invoice_date: invoice date, purchase date\n",
    "-store_name\n",
    "-store_location\n",
    "etc.\n",
    "    \n",
    "For those fields not in list must follow the Snakecase name conversation like product_name, no space allow. \n",
    "\n",
    "Output expected in JSON format as below: \n",
    "{\\\"sentiment\\\":\\\"{{Positive/Neutral/Negative}}\\\",\\\"title\\\":\\\"{{The generated title based on the input text less than 100 characters}}\\\",\\\"summary\\\":\\\"{{The generated summary based on the input text less than 300 characters}}\\\",\\\"entities\\\":[{\\\"field\\\":\\\"{{the extracted fields such as product_name listed above}}\\\",\\\"value\\\":\\\"{{the extracted value of the field}}\\\"}]}\n",
    "'''\n",
    "\n",
    "user_msg = r'''\n",
    "Input text: \n",
    "Everything was working fine one day I went to make a shot of coffee it stopped brewing after 3 seconds Then I tried the milk frother it stopped after 3 seconds again I took it back they fixed it under warranty but it’s happening again I don’t see this machine lasting more then 2 years to be honest I’m spewing I actually really like the machine It’s almost like it’s losing pressure somewhere, they wouldn’t tell my what the problem was when they fixed it.. Purchased at Harvey Norman for $1,349. \\\n",
    "Product is used: Several times a week\n",
    " \n",
    "JSON:\n",
    "'''\n",
    "\n",
    "json_data = { \n",
    "  \"model\": model,\n",
    "  \"response_format\": {\"type\": \"json_object\"}, #JSON mode\n",
    "  \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                #\"content\": \"why is the sky blue?\"\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "response = requests.post(url=openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42766769",
   "metadata": {},
   "source": [
    "Next let's constrain the structured JSON output with [BNF Grammar](https://localai.io/features/constrained_grammars/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f92492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test its openai-compatible chat completion api with to process customer message with\n",
    "# summarization, sentiment analysis and entities extraction and output as json\n",
    "\n",
    "grammar = r'''\n",
    "entities ::= \"[\" space ( entities-item ( \",\" space entities-item )* )? \"]\" space\n",
    "entities-item ::= \"{\" space entities-item-field-kv \",\" space entities-item-value-kv \"}\" space\n",
    "entities-item-field-kv ::= \"\\\"field\\\"\" space \":\" space string\n",
    "entities-item-value-kv ::= \"\\\"value\\\"\" space \":\" space string\n",
    "entities-kv ::= \"\\\"entities\\\"\" space \":\" space entities\n",
    "root ::= \"{\" space sentiment-kv \",\" space title-kv \",\" space summary-kv \",\" space entities-kv \"}\" space\n",
    "sentiment-kv ::= \"\\\"sentiment\\\"\" space \":\" space sentiment-value\n",
    "sentiment-value ::= (\"\\\"Positive\\\"\" | \"\\\"Neutral\\\"\" | \"\\\"Negative\\\"\")\n",
    "space ::= \" \"?\n",
    "string ::=  \"\\\"\" (\n",
    "        [^\"\\\\] |\n",
    "        \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F])\n",
    "      )* \"\\\"\" space\n",
    "summary-kv ::= \"\\\"summary\\\"\" space \":\" space string\n",
    "title-kv ::= \"\\\"title\\\"\" space \":\" space string\n",
    "'''\n",
    "\n",
    "json_data = { \n",
    "  \"model\": model,\n",
    "  \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                #\"content\": \"why is the sky blue?\"\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ],\n",
    "    \"grammar\": grammar\n",
    "}\n",
    "\n",
    "response = requests.post(url=openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
