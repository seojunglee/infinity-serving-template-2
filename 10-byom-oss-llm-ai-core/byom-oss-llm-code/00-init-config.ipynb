{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial configuration of SAP AI Core for BYOM-OSS-LLM-AI-CORE\n",
    "This notebook automates the initial configurations for application BYOM-OSS-LLM-AI-CORE to bring open-sourced llms into SAP AI Core. Alternatively, you can perform the same with SAP AI Launchpad.\n",
    "- Review and update the configuration in config.json\n",
    "- Initialize a client of SAP AI Core SDK\n",
    "- Create a resource group\n",
    "- Register a docker secret\n",
    "- Onboarding Git Repository and Create an Application for BYOM-OSS-LLM-AI-CORE\n",
    "- Synchronize the application and check its status\n",
    "- Create the configurations for scenarios ollama, local-ai, llama.cpp, vllm and infinity(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Copy [config.template.json](config.template.jso) as [config.json](config.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cp config.template.json config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Review and Update configuration in [config.json](config.json)\n",
    "Please read the **comments** carefully in [config.json](config.json) and update the necessary configurations.  \n",
    "- **name**: used as name of git repository and application. \n",
    "- **resource_group**: \"default\" will be used if not specified. It is optional but recommended to create a dedicate resource group, and update it [config.json](config.json). By default, \"default\" resource group is in place for all the AI Core instances.AI Core with tree tier plan is not able to create a new resource group.\n",
    "- **ai_core_sk**: update with your own AI Core Service Key\n",
    "- **docker_secret**: Update the user and password etc in .dockerconfigjson. Please refer to [this document](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/register-your-docker-registry-secret) for more detail.\n",
    "    - username: Replace <REPLACE_WITH_YOUR_DOCKER_USERNAME> with your docker user name. \n",
    "    - password: Replace <REPLACE_WITH_YOUR_DOCKER_ACCESS_TOKEN> with your docker access token.\n",
    "- **git_repo**: update the git repo configuration with your owns\n",
    "    - repo_url: url to your forked repository. It should be: https://github.com/<YOUR_GITHUB_ORG_OR_USER>/btp-generative-ai-hub-use-cases\n",
    "    - user: Update with your github user\n",
    "    - access_token: Update with your github user access token\n",
    "- **application**: The SAP AI Core application hosts the scenarios of ollama etc to serving open sourced llms in SAP AI Core\n",
    "    - path_in_repo: relative path to the serving templates. No change needed.\n",
    "- **configurations**: Review the configurations of the scenarios. By default, it is configured to load the mistral-7b quantization model with [resource plan infer.s in SAP AI Core](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/choose-resource-plan-c58d4e584a5b40a2992265beb9b6be3c) defined in [../byom-oss-llm-templates](../byom-oss-llm-templates). It is recommend to go ahead first with the default configurations in config.json.\n",
    "    - **Ollama**: By default, **[Phi3:14b](https://ollama.com/library/phi3)** model is configured. Pull the model dynamically in [ollama/ollama.ipynb](ollama/ollama.ipynb)\n",
    "    - **LocalAI**: LocalAI allows you to [preload model during startup](https://localai.io/advanced/#preloading-models-during-startup). The initial configuration in config.json will preload model [Mistral-7B-OpenOrca-GGUF](https://github.com/go-skynet/model-gallery/blob/main/mistral.yaml) with local-ai on resource plan 'infer.s' defined in [local-ai-template.yaml](../byom-oss-llm-templates/local-ai-template.yaml). In its model config file, GPU acceleration isn't enabled, hence it is quite slow. To have GPU acceleration for a model, you may set in its model config yaml file. For example [mixtral-Q6.yaml](https://github.com/go-skynet/model-gallery/blob/main/mixtral-Q6.yaml). Please review the [full config model file reference](https://localai.io/advanced/#full-config-model-file-reference)\n",
    "        ```sh\n",
    "        f16: true \n",
    "        mmap: true \n",
    "        gpu_layers: xx \n",
    "        ```\n",
    "        In addition, you can install more models through end point /model/apply in [local-ai/local-ai.ipynb](local-ai/local-ai.ipynb). Please refer to https://localai.io/advanced/#preloading-models-during-startup\n",
    "    - **llama.cpp**: By default, model [mistral-7b-instruct-v0.2.Q5_K_M.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF) is configured with alias as mistral. Unlike ollama and local-ai, llama.cpp scenario only supports one model in one configuration. If you need multiple models to be served with llama.cpp, please create multiple configurations through SAP AI Launchpad\n",
    "    - **vllm**: By default, model [TheBloke/Mistral-7B-Instruct-v0.2-AWQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-AWQ) is configured. Unlike ollama and local-ai, vllm scenario only supports one model in one configuration. If you need multiple model to be served with llama.cpp, please create multiple configurations through SAP AI Launchpad.\n",
    "    - **transformer**: By default, [Microsoft's Phi-3-vision-128k-instruct](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) is configured.\n",
    "    - **infinity**: By default, open-source text embedding model [nreimers/MiniLM-L6-H384-uncased](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) is configured. One model in per configuration. If you need multiple models to be served with infinity, please create multiple configurations through SAP AI Launchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Load the configurations from [config.json](config.json)\n",
    "The service key of AI Core are located in section ai_core_sk of [config.json](config.json).<br/>\n",
    "Please update it with your own service key before running this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations loaded from config.json\n",
      "name:  byom-open-source-llms resource_group:  oss-llm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initializations\n",
    "resource_group = config.get(\"resource_group\", \"default\")\n",
    "name = config.get(\"name\", \"open-source-llms\")\n",
    "print(\"Configurations loaded from config.json\")\n",
    "print(\"name: \", name, \"resource_group: \", resource_group )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4: Initialize AI Core SDK Client\n",
    "The service key of AI Core are located in section ai_core_sk of [config.json](config.json).<br/>\n",
    "Please update it with your own service key before running this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource group: oss-llm, name: byom-open-source-llms\n"
     ]
    }
   ],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "\n",
    "ai_core_sk = config[\"ai_core_service_key\"]\n",
    "client = AICoreV2Client(base_url=ai_core_sk.get(\"serviceurls\").get(\"AI_API_URL\")+\"/v2\",\n",
    "                        auth_url=ai_core_sk.get(\"url\")+\"/oauth/token\",\n",
    "                        client_id=ai_core_sk.get(\"clientid\"),\n",
    "                        client_secret=ai_core_sk.get(\"clientsecret\"),\n",
    "                        resource_group=resource_group)\n",
    "print(f\"resource group: {resource_group}, name: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5: Create a dedicated resource group (Optional but recommended)\n",
    "resource_group defined here must be matched with resource_group in [config.json](config.json). Default as \"oss-llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resource_group_id': 'oss-llm-2', 'labels': None, 'status': None, 'created_at': None}\n"
     ]
    }
   ],
   "source": [
    "response = client.resource_groups.create(resource_group_id = resource_group)\n",
    "print(response.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Register Docker Secret within SAP AI Core\n",
    "\n",
    "Please skip this step if you have already registered your docker secret within SAP AI Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'secret has been created'}\n",
      "Docker Registry Secret: docker-secret\n"
     ]
    }
   ],
   "source": [
    "docker_secret = config[\"docker_secret\"]\n",
    "response = client.docker_registry_secrets.create(\n",
    "    name = docker_secret[\"name\"],\n",
    "    data = docker_secret[\"data\"]\n",
    ")\n",
    "\n",
    "print(response.__dict__)\n",
    "print(f'Docker Registry Secret: {docker_secret[\"name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Update the serving templates\n",
    "Please replace the place holders in the following serving templates.\n",
    "- <YOUR_DOCKER_SECRET> to be replaced with **docker-secret** created in step 5 or your own docker secret.\n",
    "- <YOUR_DOCKER_USER> to be replaced with your own docker hub user.\n",
    "- ai.sap.com/resourcePlan: By default, the resource plan is as **infer.s**, which is sufficient for 7B model in the sample tests notebooks afterwards. If you would like to run 13B or 30B beyond etc, please use **infer.m** or **infer.l** resource plan. Check out more detail about [Resource Plan in SAP AI Core](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/choose-resource-plan-c58d4e584a5b40a2992265beb9b6be3c).\n",
    "```yaml\n",
    "    metadata:\n",
    "      #...\n",
    "      labels: |\n",
    "        ai.sap.com/resourcePlan: infer.s\n",
    "    spec: |\n",
    "      predictor:\n",
    "        imagePullSecrets:\n",
    "          - name: <YOUR_DOCKER_SECRET>\n",
    "          ...\n",
    "        containers:\n",
    "            - name: kserve-container\n",
    "              image: docker.io/<YOUR_DOCKER_USER>/ollama:ai-core\n",
    "```\n",
    "- [../byom-oss-llm-templates/llama.cpp-template.yaml](../byom-oss-llm-templates/llama.cpp-template.yaml)\n",
    "- [../byom-oss-llm-templates/local-ai-template.yaml](../byom-oss-llm-templates/local-ai-template.yaml)\n",
    "- [../byom-oss-llm-templates/ollama-template.yaml](../byom-oss-llm-templates/ollama-template.yaml)\n",
    "- [../byom-oss-llm-templates/vllm-template.yaml](../byom-oss-llm-templates/vllm-template.yaml)\n",
    "- [../byom-oss-llm-templates/infinity-template.yaml](../byom-oss-llm-templates/infinity-template.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Onboard github repository and create an application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Repository has been on-boarded.\n",
      "Id: byom-open-source-llms, Message: Application has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Onboard repository\n",
    "repo_config = config[\"git_repo\"]\n",
    "repository = client.repositories.create(name,\n",
    "                                        url=repo_config.get(\"repo_url\"),\n",
    "                                        username=repo_config.get(\"user\"),\n",
    "                                        password=repo_config.get(\"access_token\")\n",
    "                                        )\n",
    "print(repository)\n",
    "\n",
    "# Create application\n",
    "app_config = config[\"application\"]\n",
    "application = client.applications.create(revision=app_config.get(\"revision\", \"HEAD\"),\n",
    "                                        path=app_config.get(\"path_in_repo\"),\n",
    "                                        application_name=name,\n",
    "                                        repository_name=name\n",
    "                                        )\n",
    "print(application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, github repo is on-boarded in SAP AI Core.<br/>\n",
    "![github-repo-onboarded](../resources/02-onboarding-github-repo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Check if application has synced and scenario created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health Status: Healthy, Sync Status: Synced, Sync Finished at: 2024-05-09T08:36:31Z\n",
      "Application synced\n",
      "Scenario {'name': 'ollama', 'id': 'byom-ollama-server'} synced\n",
      "Scenario {'name': 'local-ai', 'id': 'byom-local-ai-server'} synced\n",
      "Scenario {'name': 'llama.cpp', 'id': 'byom-llama.cpp-server'} synced\n",
      "Scenario {'name': 'vllm', 'id': 'byom-vllm-server'} synced\n",
      "Scenario {'name': 'infinity', 'id': 'byom-infinity-server'} synced\n"
     ]
    }
   ],
   "source": [
    "max_tries = 10\n",
    "i = 0\n",
    "interval_s = 20\n",
    "while i < max_tries:\n",
    "    i = i +1\n",
    "    app_status = client.applications.get_status(name)\n",
    "    print(f\"Health Status: {app_status.health_status}, Sync Status: {app_status.sync_status}, Sync Finished at: {app_status.sync_finished_at}\" )\n",
    "    \n",
    "    if(app_status.sync_status == \"Synced\"):\n",
    "        break\n",
    "\n",
    "    # Synchronize the application and wait\n",
    "    client.applications.refresh(name) \n",
    "    sleep(interval_s)\n",
    "\n",
    "if app_status.sync_status == \"Synced\":\n",
    "    print(\"Application synced\")\n",
    "    # Check scenarios\n",
    "    scenarios = client.scenario.query()\n",
    "\n",
    "    scenario_list = config[\"scenarios\"]\n",
    "    for scenario in scenario_list:\n",
    "        scenario_name = scenario[\"name\"]\n",
    "        scenario_exists = scenario_name in [s.name for s in scenarios.resources]\n",
    "        print(f\"Scenario {scenario} synced\") if scenario_exists else print(f\"Scenario {scenario_name} not yet available\")\n",
    "\n",
    "else:\n",
    "    #print(f\"Application not yet synced after 10 time retry. Likely, something wrong in the templates under git repo {repository.url}/{app_config.get(\"path_in_repo\")}.\\nPlease check it. You can run this cell again once it is fixed.\")\n",
    "    print(f\"Application not yet synced after 10 time retry. Please execute this cell again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the scenarios are created<br/>\n",
    "![byom-oss-llm-app](../resources/03-byom-oss-llm-app-ai-launchpad.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10: Create configurations\n",
    "Create a configuration for each scenario based on [config.json](config.json) described in step 2.<br/>\n",
    "<br/>\n",
    "If you need different models from the default configuration in the matrix below, please create a configuration for the target scenario through SAP AI Launchpad.\n",
    "Scenario | Model | Resource Plan\n",
    "---------|----------|----------\n",
    "ollama   | [Phi3:14b](https://ollama.com/library/phi3) | infer.s\n",
    "local-ai | [Mistral-7B-OpenOrca-GGUF](https://github.com/go-skynet/model-gallery/blob/main/mistral.yaml) | infer.s\n",
    "llama.cpp   | [mistral-7b-instruct-v0.2.Q5_K_M.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)| infer.s\n",
    "vllm   | [TheBloke/Mistral-7B-Instruct-v0.2-AWQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-AWQ) | infer.s\n",
    "transformer   | [Microsoft's Phi-3-vision-128k-instruct](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) | infer.s\n",
    "infinity   | [nreimers/MiniLM-L6-H384-uncased](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) | infer.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function to update json file(e.g. config.json) with (key, value) pair in json\n",
    "\"\"\"\n",
    "def update_json_file(file_path, key, value):\n",
    "    # Load the JSON configuration file\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "\n",
    "    # Update the value\n",
    "    config[key] = value\n",
    "\n",
    "    # Write the updated configuration back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(config, file, indent=4)\n",
    "        print(f\"{file_path} updated. {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------byom-ollama-server--------------\n",
      "{'id': 'afb4c65a-be36-4bf4-adad-94a3ccf466d8', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: 9338f418-36be-40c9-b739-5a12ec4f853e, Message: Configuration created\n",
      "ollama/env.json updated. configuration_id: 9338f418-36be-40c9-b739-5a12ec4f853e\n",
      "--------------byom-local-ai-server--------------\n",
      "{'id': '1849757a-cd5a-4bb3-9b3c-c5717fb727a9', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: 05c671e4-5723-44ac-91e7-f769c8f1aaea, Message: Configuration created\n",
      "local-ai/env.json updated. configuration_id: 05c671e4-5723-44ac-91e7-f769c8f1aaea\n",
      "--------------byom-llama.cpp-server--------------\n",
      "{'id': 'e0be0462-0df5-48e2-a1b8-283c31db2b8c', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: d93b2fa1-5c21-447c-ba3a-d39e9fcd35c8, Message: Configuration created\n",
      "llama.cpp/env.json updated. configuration_id: d93b2fa1-5c21-447c-ba3a-d39e9fcd35c8\n",
      "--------------byom-vllm-server--------------\n",
      "{'id': '7a0fe86f-e3ad-491b-9e04-e8a325f9970c', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: 435f1738-c778-49c4-9323-66d23a138782, Message: Configuration created\n",
      "vllm/env.json updated. configuration_id: 435f1738-c778-49c4-9323-66d23a138782\n",
      "--------------byom-infinity-server--------------\n",
      "{'id': 'b08102d3-a2f4-48de-a666-f01b5f5eeb3a', 'message': 'Artifact acknowledged', 'url': 'ai://object-store-secret/Dummy'}\n",
      "Id: df6a72de-b72c-4c60-aad6-841998bacb29, Message: Configuration created\n",
      "infinity/env.json updated. configuration_id: df6a72de-b72c-4c60-aad6-841998bacb29\n",
      "config.json updated.\n"
     ]
    }
   ],
   "source": [
    "from ai_core_sdk.models import InputArtifactBinding,ParameterBinding\n",
    "from ai_api_client_sdk.models.artifact import Artifact\n",
    "from ai_api_client_sdk.models.label import Label\n",
    "\n",
    "# Create serving configurations\n",
    "conf_list = config[\"configurations\"]\n",
    "\n",
    "for conf in conf_list:\n",
    "    print(f'--------------{conf[\"scenario_id\"]}--------------')\n",
    "    # Create input artifacts for model\n",
    "    input_artifact_bindings = []\n",
    "    for ia in conf[\"input_artifacts\"]:\n",
    "        # Since it is a dummy model place holder, we only create the artifact when it is missing\n",
    "        # Otherwise, skip it.\n",
    "        if len(ia[\"artifact_id\"]) == 0:\n",
    "            artifact = client.artifact.create(\n",
    "                name = ia[\"name\"], # Custom Non-unqiue identifier\n",
    "                kind = Artifact.Kind.MODEL,\n",
    "                url = ia[\"url\"], \n",
    "                scenario_id = conf[\"scenario_id\"],\n",
    "                description = ia[\"description\"]\n",
    "                # labels = [\n",
    "                #      Label(key=\"ext.ai.sap.com/model\", value=ia[\"name\"]) # any descriptive key-value pair, helps in filtering, key must have the prefix ext.ai.sap.com/\n",
    "                # ]\n",
    "            )\n",
    "            \n",
    "            print(artifact.__dict__)\n",
    "            # Write back the artifact_id to configuration\n",
    "            ia[\"artifact_id\"] = artifact.id\n",
    "        input_artifact_bindings.append(InputArtifactBinding(key=ia['key'], artifact_id=ia[\"artifact_id\"]))\n",
    "    parameter_bindings = [ParameterBinding(key=pb['key'], value=pb['value']) for pb in conf[\"parameters\"]] \n",
    "  \n",
    "    # Create the configuration with associated parameters and input artifacts\n",
    "    configuration = client.configuration.create(\n",
    "        name=conf[\"name\"],\n",
    "        scenario_id=conf[\"scenario_id\"],\n",
    "        executable_id=conf[\"executable_id\"],\n",
    "        parameter_bindings=parameter_bindings,\n",
    "        input_artifact_bindings = input_artifact_bindings\n",
    "    )\n",
    "    \n",
    "    print(configuration)\n",
    "\n",
    "    # Update the configuration_id in env.json under the corresponding folder\n",
    "    # which will be used in continuos-deployment.ipynb to create deployment automatically.\n",
    "    update_json_file(f'{conf[\"executable_id\"]}/env.json',\"configuration_id\", configuration.id)\n",
    "    config_id = configuration.id\n",
    "\n",
    "# write back to config.json\n",
    "with open(\"config.json\", 'w') as file:\n",
    "        json.dump(config, file, indent=4)\n",
    "        print(\"config.json updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
