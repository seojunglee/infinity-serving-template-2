apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: local-ai
  annotations:
    scenarios.ai.sap.com/description: "Run a LocalAI server on SAP AI Core"
    scenarios.ai.sap.com/name: "local-ai"
    executables.ai.sap.com/description: "Run a LocalAI server on SAP AI Core"
    executables.ai.sap.com/name: "local-ai"
  labels:
    scenarios.ai.sap.com/id: "local-ai"
    ai.sap.com/version: "0.0.1"
spec:
  inputs:
    parameters:
      - name: model_name
        default: "mistral"
        type: string
        description: "Model Name to be used for inference with SAP Generative AI Hub SDK"
      - name: preloaded_models
        default: "[{'id': 'model-gallery@mistral'}]" 
        type: string
        description: "Preloading models during startup.Refer to https://localai.io/advanced/#preloading-models-during-startup for detail"
      - name: debug
        default: "true"
        type: string
        description: "Enable debug or not. Valid value as true or false"
      - name: resource_plan
        type: "string"
        default: "infer.s"
        description: "Resource Plan of SAP AI Core. Supported: infer.s, infer.m, infer.l, train.l"
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    metadata:
      annotations: |
        autoscaling.knative.dev/metric: concurrency
        autoscaling.knative.dev/target: 1
        autoscaling.knative.dev/targetBurstCapacity: 0
      labels: |
        ai.sap.com/resourcePlan: "{{inputs.parameters.resource_plan}}"
    spec: |
      predictor:
        imagePullSecrets:
        - name: <YOUR_DOCKER_SECRET>
        minReplicas: 1
        maxReplicas: 1
        containers:
        - name: kserve-container
          image: docker.io/<YOUR_DOCKER_USER>/local-ai:ai-core
          ports:
            - containerPort: 18080
              protocol: TCP
          env:
            - name: PRELOAD_MODELS
              value: "{{inputs.parameters.preloaded_models}}"
            - name: DEBUG
              value: "{{inputs.parameters.debug}}"