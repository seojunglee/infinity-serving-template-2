{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a5a74d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook, we will test out a mistral-7b with [vLLM](https://docs.vllm.ai/) in SAP AI Core. You can also run Llama 2, Mixtral, Gemma, and other [supported models](https://docs.vllm.ai/en/latest/models/supported_models.html). \n",
    "\n",
    "### Prerequisites\n",
    "Before running this notebook, please assure you have performed the [Prerequisites](../../README.md) and [01-deployment.ipynb](01-deployment.ipynb). As a result, a deployment of vLLM scenario is running in SAP AI Core.<br/><br/>\n",
    "\n",
    "If the configuration and deployment are created through SAP AI Launchpad, please manually update the configuration_id and deployment_id in [env.json](env.json)\n",
    "```json\n",
    "{\n",
    "    \"configuration_id\": \"<YOUR_CONFIGURATION_ID_OF_VLLM_SCENARIO>\",\n",
    "    \"deployment_id\": \"<YOUR_DEPLOYMENT_ID_BASED_ON_CONFIG_ABOVE>\"\n",
    "}\n",
    "```\n",
    " \n",
    "### The high-level flow:\n",
    "- Load configurations info\n",
    "- Connect to SAP AI Core via SDK\n",
    "- Check the status and logs of the deployment\n",
    "- Inference the model with OpenAI-compatible chat completion API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55bd7b",
   "metadata": {},
   "source": [
    "#### 1.Load config info \n",
    "- resource_group loaded from [config.json](../config.json)\n",
    "- deployment_id(created in 01-deployment.ipynb) loaded [env.json](env.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f1e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from ai_api_client_sdk.ai_api_v2_client import AIAPIV2Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eee26b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment id:  d2205e6da6740a73  resource group:  oss-llm\n"
     ]
    }
   ],
   "source": [
    "# Please replace the configurations below.\n",
    "# config_id: The target configuration to create the deployment. Please create the configuration first.\n",
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "with open(\"./env.json\") as f:\n",
    "    env = json.load(f)\n",
    "\n",
    "deployment_id = env[\"deployment_id\"]\n",
    "resource_group = config.get(\"resource_group\", \"default\")\n",
    "print(\"deployment id: \", deployment_id, \" resource group: \", resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd694c3",
   "metadata": {},
   "source": [
    "#### 2.Initiate connection to SAP AI Core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4cc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_sk = config[\"ai_core_service_key\"]\n",
    "base_url = aic_sk[\"serviceurls\"][\"AI_API_URL\"] + \"/v2/lm\"\n",
    "ai_api_client = AIAPIV2Client(\n",
    "    base_url= base_url,\n",
    "    auth_url=aic_sk[\"url\"] + \"/oauth/token\",\n",
    "    client_id=aic_sk['clientid'],\n",
    "    client_secret=aic_sk['clientsecret'],\n",
    "    resource_group=resource_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ffb297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = ai_api_client.rest_client.get_token()\n",
    "headers = {\n",
    "        \"Authorization\": token,\n",
    "        'ai-resource-group': resource_group,\n",
    "        \"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7b416",
   "metadata": {},
   "source": [
    "#### 3.Check the deployment status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46cf76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment-d2205e6da6740a73 is running. Ready for inference request\n"
     ]
    }
   ],
   "source": [
    "# Check deployment status before inference request\n",
    "deployment_url = f\"{base_url}/deployments/{deployment_id}\"\n",
    "response = requests.get(url=deployment_url, headers=headers)\n",
    "resp = response.json()    \n",
    "status = resp['status']\n",
    "\n",
    "deployment_log_url = f\"{base_url}/deployments/{deployment_id}/logs\"\n",
    "if status == \"RUNNING\":\n",
    "        print(f\"Deployment-{deployment_id} is running. Ready for inference request\")\n",
    "else:\n",
    "        print(f\"Deployment-{deployment_id} status: {status}. Not yet ready for inference request\")\n",
    "        #retrieve deployment logs\n",
    "        #{{apiurl}}/v2/lm/deployments/{{deploymentid}}/logs.\n",
    "\n",
    "        response = requests.get(deployment_log_url, headers=headers)\n",
    "        print('Deployment Logs:\\n', response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7d13c",
   "metadata": {},
   "source": [
    "#### 4.Inference completion and chat completion APIs\n",
    "As of 16 March 2024  \n",
    "- vllm v0.3.3, based docker image: [vllm/vllm-openai:v0.3.3](https://hub.docker.com/layers/vllm/vllm-openai/v0.3.3/images/sha256-4aea20de3b421f7775cfdc6468a04a29d0fcfc3603ad3b18aab4ef1f4652769d?context=explore)\n",
    "- SAP AI Core [resource plan](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/choose-resource-plan-c58d4e584a5b40a2992265beb9b6be3c)\n",
    "\n",
    "##### Test summary for TheBloke/Mistral-7B-Instruct-v0.2\n",
    "- SAP AI Core resource plan: infer-S (cuda Out-of-Memory for unquantized model, works well for awq quantized model)\n",
    "- Failed to load with out of memory with unquantized model TheBloke/Mistral-7B-Instruct-v0.2 with cuda out of memory which seems to require too much GPU VRAM.\n",
    "- Have tried the suggested options below without luck:\n",
    "    ```sh\n",
    "    --gpu-memory-utilization 0.95 #(also tried from 0.4 to 0.95) \n",
    "    --enforce-eager\n",
    "    --max-model-len 2048\n",
    "    --max-num-batched-tokens 2048\n",
    "    --max-num-seqs 2048\n",
    "    ```\n",
    "- Likely it is a bug according to this issue(https://github.com/vllm-project/vllm/issues/2248). \n",
    "- Hence switch AWQ quantization model(TheBloke/Mistral-7B-Instruct-v0.2-AWQ). It works well with same configuration above.<b4/>\n",
    "\n",
    "##### Test summary for **TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ**\n",
    "- SAP AI Core resource plan infer-L(cuda Out-of-Memory for unquantized model)\n",
    "- Failed to load with out of memory with awq model TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ with cuda out of memory which seems to require too much GPU VRAM.\n",
    "- Have tried the suggested options below without luck:\n",
    "    ```sh\n",
    "    --gpu-memory-utilization 0.95 #(also tried from 0.4 to 0.95) \n",
    "    --enforce-eager\n",
    "    --max-model-len 512\n",
    "    --max-num-batched-tokens 512\n",
    "    --max-num-seqs 512\n",
    "    ```\n",
    "- Likely it is a bug according to this issue(https://github.com/vllm-project/vllm/issues/2248). \n",
    "<br/><br/>\n",
    "\n",
    "**Important**: <br/>\n",
    "Please choose your target model with [hugging face](https://huggingface.co) model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0658246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "#model = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ\" #not working\n",
    "\n",
    "deployment = ai_api_client.deployment.get(deployment_id)\n",
    "inference_base_url = f\"{deployment.deployment_url}\"\n",
    "openai_chat_api_endpoint = f\"{inference_base_url}/v1/chat/completions\"\n",
    "openai_completion_api_endpoint = f\"{inference_base_url}/v1/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97029db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d2205e6da6740a73/v1/models\n",
      "Result: {\"object\":\"list\",\"data\":[{\"id\":\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\"object\":\"model\",\"created\":1711597462,\"owned_by\":\"vllm\",\"root\":\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\"parent\":null,\"permission\":[{\"id\":\"modelperm-c20740c70a124f208331e62e530bb4e5\",\"object\":\"model_permission\",\"created\":1711597462,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}\n"
     ]
    }
   ],
   "source": [
    "# List models\n",
    "endpoint = f\"{inference_base_url}/v1/models\"\n",
    "print(endpoint)\n",
    "\n",
    "response = requests.get(url=endpoint, headers=headers)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848db40",
   "metadata": {},
   "source": [
    "#### 4.1 Sample#1: Test OpenAI-compatible API for Chat Completion\n",
    "Now let's test its [OpenAI-compatible API for Chat Completion](https://github.com/ggerganov/llama.cpp/tree/master/examples/server) with a basic sample about Chain of Thought, which is the exact API interface of Chat Completion of GPT-3.5/4 in SAP Generative AI Hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3725c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"id\":\"cmpl-0725f1ceafb64301912218e8d4ced5ad\",\"object\":\"chat.completion\",\"created\":9457,\"model\":\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\" Roger initially has 5 tennis balls. He then buys 2 cans, each containing 3 tennis balls. So, he gets 2 * 3 = <<2*3=6>>6 tennis balls from the cans.\\n\\nTherefore, Roger now has 5 (his initial balls) + 6 (new balls from cans) = <<5+6=11>>11 tennis balls.\"},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":53,\"total_tokens\":142,\"completion_tokens\":89}}\n"
     ]
    }
   ],
   "source": [
    "#let's try its openai-compatible chat completion api\n",
    "sys_msg = \"You are an helpful AI assistant\"\n",
    "#user_msg = \"why the sky is blue?\"\n",
    "user_msg = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?Let's thinks step by step.\"\n",
    "json_data = { \n",
    "  \"model\": model, \n",
    "  \"messages\": [\n",
    "            # mistral fine tune model doesn't accept system message. \n",
    "            # please refer to https://github.com/vllm-project/vllm/discussions/2112\n",
    "            # {\n",
    "            #     \"role\": \"system\",\n",
    "            #     \"content\": sys_msg\n",
    "            # },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "response = requests.post(openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c9f16",
   "metadata": {},
   "source": [
    "#### 4.2 Sample#2: Write a haiku about running vllm in AI Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c824efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'id': 'cmpl-e3d38647b9d241ccbb10159c92f7eb94', 'object': 'chat.completion', 'created': 8020, 'model': 'TheBloke/Mistral-7B-Instruct-v0.2-AWQ', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': ' Silicon heart beats,\\n\\nData streams in endless dance,\\nVlm runs, learning grows.'}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 21, 'total_tokens': 44, 'completion_tokens': 23}}\n"
     ]
    }
   ],
   "source": [
    "#let's test its openai-compatible chat completion api by writing a haiku\n",
    "user_msg = \"Write a haiku for running vllm in AI Core\"\n",
    "json_data = {\n",
    "  \"model\": model,\n",
    "  \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "response = requests.post(openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b4ab1",
   "metadata": {},
   "source": [
    "#### 4.3 Sample#3: Customer Message Processing \n",
    "In our sample [btp-industry-use-cases/04-customer-interaction-gpt4](https://github.com/SAP-samples/btp-industry-use-cases/tree/main/04-customer-interaction-gpt4),GPT-3.5/4 is used to process customer messages in customer interactions and output in json schema with plain prompting.\n",
    "- Summarize customer message into title and a short description\n",
    "- Analyze the sentiment of the customer message\n",
    "- Extract the entities from the customer message, such as customer, product, order no etc.\n",
    "\n",
    "Let's see if the same scenario could be achieved with mistral-7b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74cfc90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {\"id\":\"cmpl-b514ea60b7ac4b2a9a96b84546d7604a\",\"object\":\"chat.completion\",\"created\":8097,\"model\":\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\" {\\n\\\"sentiment\\\": \\\"Negative\\\",\\n\\\"title\\\": \\\"Issues with Coffee Machine: Brewing and Milk Frothing Stop After Short Time\\\",\\n\\\"summary\\\": \\\"The coffee machine stopped brewing and milk frothing after a few seconds. It was fixed under warranty but the issue recurred. Machine is suspected to have pressure loss problem. Customer expresses dissatisfaction and doubts machine's longevity. Purchased at Harvey Norman for $1,349.\\\",\\n\\\"entities\\\": [\\n{\\\"field\\\": \\\"product_name\\\", \\\"value\\\": \\\"coffee machine\\\"},\\n{\\\"field\\\": \\\"customer_no\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"customer_name\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"customer_phone\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"product_no\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"order_no\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"order_date\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"delivery_no\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"delivery_date\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"invoice_no\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"invoice_date\\\", \\\"value\\\": \\\"\\\"},\\n{\\\"field\\\": \\\"store_name\\\", \\\"value\\\": \\\"Harvey Norman\\\"},\\n{\\\"field\\\": \\\"product_price\\\", \\\"value\\\": \\\"$1,349\\\"}\\n]\\n}\"},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":577,\"total_tokens\":888,\"completion_tokens\":311}}\n"
     ]
    }
   ],
   "source": [
    "# Let's test its openai-compatible chat completion api with to process customer message with\n",
    "# summarization, sentiment analysis and entities extraction and output as json\n",
    "user_msg = r'''\n",
    "You are an AI assistant to process the input text. Here are your tasks on the text.\n",
    "1.Apply Sentiment Analysis\n",
    "2.Generate a title less than 100 characters,and summarize the text into a short description less than 200 characters\n",
    "3.Extract the entities such as customer,product,order,delivery,invoice etc from the text Here is a preliminary list of the target entity fields and description. Please extract all the identifiable entities even not in the list below. Don't include any field with unknown value. \\\n",
    "-customer_no: alias customer number, customer id, account id, account number which could be used to identify a customer.\n",
    "-customer_name: customer name, account name\n",
    "-customer_phone: customer contact number. -product_no: product number, product id\n",
    "-product_name\n",
    "-order_no: sales order number, order id\n",
    "-order_date \n",
    "-delivery_no: delivery number, delivery id\n",
    "-delivery_date: delivery date, shipping date\n",
    "-invoice_no: alias invoice number, invoice id, receipt number, receipt id etc. which can be used to locate a invoice.\n",
    "-invoice_date: invoice date, purchase date\n",
    "-store_name\n",
    "-store_location\n",
    "etc.\n",
    "    \n",
    "For those fields not in list must follow the Snakecase name conversation like product_name, no space allow. \n",
    "\n",
    "Output expected in JSON format as below: \n",
    "{\\\"sentiment\\\":\\\"{{Positive/Neutral/Negative}}\\\",\\\"title\\\":\\\"{{The generated title based on the input text less than 100 characters}}\\\",\\\"summary\\\":\\\"{{The generated summary based on the input text less than 300 characters}}\\\",\\\"entities\\\":[{\\\"field\\\":\\\"{{the extracted fields such as product_name listed above}}\\\",\\\"value\\\":\\\"{{the extracted value of the field}}\\\"}]}\n",
    "\n",
    "Input text: \n",
    "Everything was working fine one day I went to make a shot of coffee it stopped brewing after 3 seconds Then I tried the milk frother it stopped after 3 seconds again I took it back they fixed it under warranty but it’s happening again I don’t see this machine lasting more then 2 years to be honest I’m spewing I actually really like the machine It’s almost like it’s losing pressure somewhere, they wouldn’t tell my what the problem was when they fixed it.. Purchased at Harvey Norman for $1,349. \\\n",
    "Product is used: Several times a week\n",
    " \n",
    "JSON:\n",
    "'''\n",
    "\n",
    "json_data = { \n",
    "  \"model\": model,\n",
    "  \"response_format\": {\"type\": \"json_object\"}, #JSON mode\n",
    "  \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "}\n",
    "\n",
    "response = requests.post(url=openai_chat_api_endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
