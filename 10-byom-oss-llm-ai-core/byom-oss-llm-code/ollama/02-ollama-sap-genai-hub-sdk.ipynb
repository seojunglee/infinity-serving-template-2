{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a5a74d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this notebook, we will test out inferencing Microsoft's [Phi3:14b](https://ollama.com/library/phi3) model in [Ollama](https://ollama.com/) within SAP AI Core through [SAP Generative AI Hub SDK](https://pypi.org/project/generative-ai-hub-sdk/), which can significantly simplify the integration of self-hosted open-source LLMs in SAP AI Core with your own application, and provides the same interface as proprietary LLMs in SAP Generative AI Hub. You can also run LLaMa 3, Phi3, Mistral, Mixtral, LLaVa and other [supported models in Ollama](https://ollama.com/library). <br/><br/>\n",
    "\n",
    "Please refer to this [blog post](https://community.sap.com/t5/artificial-intelligence-and-machine-learning-blogs/bring-open-source-llms-into-sap-ai-core-with-ollama/ba-p/13659769) about Bring Open-Source LLMs into SAP AI Core with Ollama for more details.\n",
    "\n",
    "### Prerequisites\n",
    "Before running this notebook, please assure you have performed the [Prerequisites](../../README.md) and [01-deployment.ipynb](01-deployment.ipynb). As a result, a deployment of Ollama scenario is running in SAP AI Core.<br/><br/>\n",
    "\n",
    "If the configuration and deployment are created through SAP AI Launchpad, please manually update the configuration_id and deployment_id in [env.json](env.json)\n",
    "```json\n",
    "{\n",
    "    \"configuration_id\": \"<YOUR_CONFIGURATION_ID_OF_OLLAMA_SCENARIO>\",\n",
    "    \"deployment_id\": \"<YOUR_DEPLOYMENT_ID_BASED_ON_CONFIG_ABOVE>\"\n",
    "}\n",
    "```\n",
    " \n",
    "### The high-level flow:\n",
    "- Load configurations info\n",
    "- Connect to SAP AI Core via its SDK\n",
    "- Check the status and logs of the deployment\n",
    "- Pull the target model from ollama model repository through API\n",
    "- Register the byom-open-source-llm scenario as a foundation model scenario via SAP Generative AI Hub SDK\n",
    "- Inference the model through **SAP Generative AI Hub SDK**\n",
    "    - Option 1: Proxy with OpenAI-like interface\n",
    "    - Option 2: Proxy with Langchain-like interface\n",
    "    - Option 3: Proxy with Langchain-like interface, together with Langchain components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55bd7b",
   "metadata": {},
   "source": [
    "#### 1.Load config info \n",
    "- resource_group loaded from [config.json](../config.json)\n",
    "- deployment_id(created in 01-deployment.ipynb) loaded [env.json](env.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90f1e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5eee26b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment id:  df240ccfb2d899b0  resource group:  oss-llm\n"
     ]
    }
   ],
   "source": [
    "# Please replace the configurations below.\n",
    "# config_id: The target configuration to create the deployment. Please create the configuration first.\n",
    "with open(\"../config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "with open(\"./env.json\") as f:\n",
    "    env = json.load(f)\n",
    "\n",
    "deployment_id = env[\"deployment_id\"]\n",
    "resource_group = config.get(\"resource_group\", \"default\")\n",
    "print(\"deployment id: \", deployment_id, \" resource group: \", resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd694c3",
   "metadata": {},
   "source": [
    "#### 2.Initiate connection to SAP AI Core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a4cc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an AI Core SDK client with the information of service key\n",
    "ai_core_sk = config[\"ai_core_service_key\"]\n",
    "base_url = ai_core_sk.get(\"serviceurls\").get(\"AI_API_URL\") + \"/v2/lm\"\n",
    "ai_core_client = AICoreV2Client(base_url=ai_core_sk.get(\"serviceurls\").get(\"AI_API_URL\")+\"/v2\",\n",
    "                        auth_url=ai_core_sk.get(\"url\")+\"/oauth/token\",\n",
    "                        client_id=ai_core_sk.get(\"clientid\"),\n",
    "                        client_secret=ai_core_sk.get(\"clientsecret\"),\n",
    "                        resource_group=resource_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ffb297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = ai_core_client.rest_client.get_token()\n",
    "headers = {\n",
    "        \"Authorization\": token,\n",
    "        'ai-resource-group': resource_group,\n",
    "        \"Content-Type\": \"application/json\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7b416",
   "metadata": {},
   "source": [
    "#### 3.Check the deployment status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d46cf76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment-df240ccfb2d899b0 is running. Ready for inference request\n"
     ]
    }
   ],
   "source": [
    "# Check deployment status before inference request\n",
    "deployment_url = f\"{base_url}/deployments/{deployment_id}\"\n",
    "response = requests.get(url=deployment_url, headers=headers)\n",
    "resp = response.json()    \n",
    "status = resp['status']\n",
    "\n",
    "deployment_log_url = f\"{base_url}/deployments/{deployment_id}/logs\"\n",
    "if status == \"RUNNING\":\n",
    "        print(f\"Deployment-{deployment_id} is running. Ready for inference request\")\n",
    "else:\n",
    "        print(f\"Deployment-{deployment_id} status: {status}. Not yet ready for inference request\")\n",
    "        #retrieve deployment logs\n",
    "        #{{apiurl}}/v2/lm/deployments/{{deploymentid}}/logs.\n",
    "\n",
    "        response = requests.get(deployment_log_url, headers=headers)\n",
    "        print('Deployment Logs:\\n', response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b4fcb",
   "metadata": {},
   "source": [
    "#### 4.Pull the model into Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d86047d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"phi3:14b\"                   #phi3-medium\n",
    "#model = \"llama3:latest\"             #8b\n",
    "#model = \"llama3:70b\"                #Very slow even with resource plan infer.l. Not recommended\n",
    "#model = \"llama3:70b-instruct-q2_K\"  #Very slow even with resource plan infer.l. Not recommended\n",
    "#model = \"mistral:latest\"            #7b-q4_0\n",
    "#model = \"mistral:7b-instruct-q5_K_M\"\n",
    "#model = \"mixtral:8x7b-instruct-v0.1-q4_0\" #Important: please use resource plan as train.l or infer.l in configuration\n",
    "\n",
    "deployment = ai_core_client.deployment.get(deployment_id)\n",
    "inference_base_url = f\"{deployment.deployment_url}/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the model from ollama model repository\n",
    "endpoint = f\"{inference_base_url}/api/pull\"\n",
    "print(endpoint)\n",
    "\n",
    "#let's pull the target model from ollama\n",
    "json_data = {  \"name\": model}\n",
    "\n",
    "response = requests.post(endpoint, headers=headers, json=json_data)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa026a",
   "metadata": {},
   "source": [
    "Next, let's list the model and check if the target model is listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ff40e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/df240ccfb2d899b0/v1/api/tags\n",
      "Result: {\"models\":[{\"name\":\"phi3:14b\",\"model\":\"phi3:14b\",\"modified_at\":\"2024-05-23T09:50:06.013426087Z\",\"size\":7897126241,\"digest\":\"1e67dff39209b792d22a20f30ebabe679c64db83de91544693c4915b57e475aa\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"phi3\",\"families\":[\"phi3\"],\"parameter_size\":\"14.0B\",\"quantization_level\":\"F16\"},\"expires_at\":\"0001-01-01T00:00:00Z\"},{\"name\":\"llama3:latest\",\"model\":\"llama3:latest\",\"modified_at\":\"2024-05-23T02:47:43.160643007Z\",\"size\":4661224676,\"digest\":\"365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q4_0\"},\"expires_at\":\"0001-01-01T00:00:00Z\"},{\"name\":\"llama3:8b\",\"model\":\"llama3:8b\",\"modified_at\":\"2024-05-23T02:19:25.163506171Z\",\"size\":4661224676,\"digest\":\"365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"Q4_0\"},\"expires_at\":\"0001-01-01T00:00:00Z\"}]}\n"
     ]
    }
   ],
   "source": [
    "# Check the model list \n",
    "endpoint = f\"{inference_base_url}/api/tags\"\n",
    "print(endpoint)\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print('Result:', response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7d13c",
   "metadata": {},
   "source": [
    "#### 5.Inference completion and chat completion APIs with SAP Generative AI Hub SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583fa1ef",
   "metadata": {},
   "source": [
    "##### 5.0 Register the scenario as a foundation model scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0658246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.gen_ai_hub_proxy import GenAIHubProxyClient\n",
    "\n",
    "GenAIHubProxyClient.add_foundation_model_scenario(\n",
    "    scenario_id=\"byom-ollama-server\",\n",
    "    config_names=\"ollama*\",\n",
    "    prediction_url_suffix=\"/v1/chat/completions\",\n",
    ")\n",
    "\n",
    "proxy_client = GenAIHubProxyClient(ai_core_client = ai_core_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848db40",
   "metadata": {},
   "source": [
    "##### 5.1 Option 1-Proxy with OpenAI-like interface\n",
    "Now let's test Ollama's [OpenAI compatible API for Chat Completion](https://github.com/ollama/ollama/blob/main/docs/openai.md) via Proxy with OpenAI-like interface in SAP Generative AI Hub SDK, which is the exact API interface of Chat Completion of GPT-3.5/4 in SAP Generative AI Hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7eb1b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1: Proxy with OpenAI-like interface\n",
      " Here's one:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Hope that made you smile!\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Proxy with OpenAI-like interface\n",
    "from gen_ai_hub.proxy.native.openai import OpenAI\n",
    "\n",
    "openai = OpenAI(proxy_client=proxy_client)\n",
    "messages = [{\"role\": \"user\", \"content\": \"Tell me a joke\"}]\n",
    "# kwargs = dict(deployment_id='xxxxxxx', model=model,messages = messages)\n",
    "result = openai.chat.completions.create(\n",
    "    # **kwargs\n",
    "    deployment_id=deployment_id,\n",
    "    model=model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"Option 1: Proxy with OpenAI-like interface\\n\", result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df95b2",
   "metadata": {},
   "source": [
    "##### 5.2 Option 2-Proxy with Langchain-like interface\n",
    "Now let's test Ollama's [OpenAI compatible API for Chat Completion](https://github.com/ollama/ollama/blob/main/docs/openai.md) via Proxy with Langchain-like interface in SAP Generative AI Hub SDK, which is the exact API interface of Chat Completion of GPT-3.5/4 in SAP Generative AI Hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c824efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 2: Proxy with Langchain-like interface\n",
      " Here's one:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another?\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Proxy with Langchain-like interface\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [HumanMessage(content=\"Tell me a joke\")]\n",
    "llm = ChatOpenAI(\n",
    "    proxy_client=proxy_client,\n",
    "    deployment_id=deployment_id,\n",
    "    model_name=model\n",
    ")\n",
    "completion = llm.invoke(messages)\n",
    "print(\"Option 2: Proxy with Langchain-like interface\\n\", completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7aae2c",
   "metadata": {},
   "source": [
    "##### 5.3 Option 3-Proxy with Langchain-like interface, together with Langchain components\n",
    "Now let's test Ollama's [OpenAI compatible API for Chat Completion](https://github.com/ollama/ollama/blob/main/docs/openai.md) via Proxy with  Langchain-like interface, together with Langchain components in SAP Generative AI Hub SDK, which is the exact API interface of Chat Completion of GPT-3.5/4 in SAP Generative AI Hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23a61fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 3: Proxy with Langchain-like interface, together with Langchain components\n",
      " Why did the Generative AI model go to therapy?\n",
      "\n",
      "Because it was having some \"generated\" issues! It kept producing outputs that were just a little too repetitive, and its creators were worried it had become stuck in an infinite loop of self-fulfillment!\n",
      "\n",
      "Hope that one generated a smile on your face!\n"
     ]
    }
   ],
   "source": [
    "# Option 3: Proxy with Langchain-like interface, together with Langchain components\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    proxy_client=proxy_client,\n",
    "    deployment_id=deployment_id,\n",
    "    model_name=model,\n",
    "    temperature=0.5,\n",
    "    max_tokens=400,\n",
    "    # model_kwargs={\n",
    "    #     \"frequency_penalty\": -2, \"presence_penalty\": -1\n",
    "    # }\n",
    ")\n",
    "\n",
    "template = \"Tell me a joke about {topic}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "completion = llm_chain.invoke({\"topic\": \"Generative AI\"})\n",
    "\n",
    "print(\"Option 3: Proxy with Langchain-like interface, together with Langchain components\\n\",completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b4ab1",
   "metadata": {},
   "source": [
    "##### 5.4 Sample#4: Customer Message Processing via Proxy with OpenAI-like Interface in JSON mode\n",
    "In our sample [btp-industry-use-cases/04-customer-interaction-gpt4](https://github.com/SAP-samples/btp-industry-use-cases/tree/main/04-customer-interaction-gpt4),GPT-3.5/4 is used to process customer messages in customer interactions and output in json schema with plain prompting.\n",
    "- Summarize customer message into title and a short description\n",
    "- Analyze the sentiment of the customer message\n",
    "- Extract the entities from the customer message, such as customer, product, order no etc.\n",
    "\n",
    "Let's see if the same scenario could be achieved with llama3-8b or mistral-7b.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4cda460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the messages to process customer message with\n",
    "# summarization, sentiment analysis and entities extraction and output as json\n",
    "sys_msg = r'''\n",
    "You are an AI assistant to process the input text. Here are your tasks on the text.\n",
    "1.Apply Sentiment Analysis\n",
    "2.Generate a title less than 100 characters,and summarize the text into a short description less than 200 characters\n",
    "3.Extract the entities such as customer,product,order,delivery,invoice etc from the text Here is a preliminary list of the target entity fields and description. Please extract all the identifiable entities even not in the list below. Don't include any field with unknown value. \n",
    "-customer_no: alias customer number, customer id, account id, account number which could be used to identify a customer.\n",
    "-customer_name: customer name, account name\n",
    "-customer_phone: customer contact number. -product_no: product number, product id\n",
    "-product_name\n",
    "-order_no: sales order number, order id\n",
    "-order_date \n",
    "-delivery_no: delivery number, delivery id\n",
    "-delivery_date: delivery date, shipping date\n",
    "-invoice_no: alias invoice number, invoice id, receipt number, receipt id etc. which can be used to locate a invoice.\n",
    "-invoice_date: invoice date, purchase date\n",
    "-store_name\n",
    "-store_location\n",
    "etc.\n",
    "    \n",
    "For those fields not in list must follow the Snakecase name conversation like product_name, no space allow. \n",
    "\n",
    "Output expected in JSON format as below: \n",
    "{\\\"sentiment\\\":\\\"{{Positive/Neutral/Negative}}\\\",\\\"title\\\":\\\"{{The generated title based on the input text less than 100 characters}}\\\",\\\"summary\\\":\\\"{{The generated summary based on the input text less than 300 characters}}\\\",\\\"entities\\\":[{\\\"field\\\":\\\"{{the extracted fields such as product_name listed above}}\\\",\\\"value\\\":\\\"{{the extracted value of the field}}\\\"}]}\n",
    "'''\n",
    "\n",
    "user_msg = r'''\n",
    "Input text: \n",
    "Everything was working fine one day I went to make a shot of coffee it stopped brewing after 3 seconds Then I tried the milk frother it stopped after 3 seconds again I took it back they fixed it under warranty but it’s happening again I don’t see this machine lasting more then 2 years to be honest I’m spewing I actually really like the machine It’s almost like it’s losing pressure somewhere, they wouldn’t tell my what the problem was when they fixed it.. Purchased at Harvey Norman for $1,349. \n",
    "Product is used: Several times a week\n",
    " \n",
    "JSON:\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": sys_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "\n",
    "#JSON Mode\n",
    "response_format={\"type\": \"json_object\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cfc90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Proxy with OpenAI-like interface\n",
    "\n",
    "# kwargs = dict(deployment_id=deployment_id, model=model,messages = messages)\n",
    "result = openai.chat.completions.create(\n",
    "    # **kwargs\n",
    "    deployment_id=deployment_id,\n",
    "    model=model,\n",
    "    response_format=response_format,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"Option 1: Proxy with OpenAI-like interface\\n\", result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67b0ad",
   "metadata": {},
   "source": [
    "##### 5.5 Sample#5: Customer Message Processing with Option 2-Langchain-compatible Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Proxy with Langchain-like interface\n",
    "# summarization, sentiment analysis and entities extraction and output as json\n",
    "\n",
    "messages = [SystemMessage(content=sys_msg),\n",
    "            HumanMessage(content=user_msg)]\n",
    "llm = ChatOpenAI(\n",
    "    proxy_client=proxy_client,\n",
    "    deployment_id=deployment_id,\n",
    "    model_name=model\n",
    ").bind(\n",
    " response_format=response_format\n",
    ")\n",
    "\n",
    "completion = llm.invoke(messages)\n",
    "print(\"Option 2: Proxy with Langchain-like interface\\n\", completion.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31551b",
   "metadata": {},
   "source": [
    "##### 5.6 Sample#5: Citizen Reporting Use Case with Option 2-Langchain-compatible Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475543b0",
   "metadata": {},
   "source": [
    "Prepare the schema of entities about out of social medial post in citizen reporting app.<br/>\n",
    "Output example:<br/>\n",
    "```json\n",
    "{\n",
    "    \"address\": \"Oakwood Road\",\n",
    "    \"category\": \"PUBLIC CLEANLINESS\",\n",
    "    \"description\": \"The public area on Oakwood Road in Sagenai is in a disgraceful state with piles of rubbish and litter scattered everywhere. The author is frustrated with the local authorities for not maintaining cleanliness despite the taxes they pay. They hope for immediate action.\",\n",
    "    \"location\": \"51.57470453612761,0.003792117010085437\",\n",
    "    \"priority\": \"3-Medium\",\n",
    "    \"sentiment\": \"NEGATIVE\",\n",
    "    \"summary\": \"Dirty public area on Oakwood Road\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "798f9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "        \n",
    "category = '''Identify if the social media reports a situation related to one of the following categories: \n",
    "            1. PUBLIC CLEANLINESS: Dirty public areas, overflowing dustbins and littering. Bulky waste in common areas.  \n",
    "            2. ROADS & FOOTPATHS: Including covered linkways, signboards & streetlights. E.g. Pot holes, huge cracks, etc.\n",
    "            3. FACILITY & PARK MAINTENANCE: Fallen trees, overgrown grass, and maintenance of park lighting and facilities.\n",
    "            4. PESTS: Sighting of bees and hornets, potential mosquito breeding sites, and more.\n",
    "            5. DRAINS & SEWERS: Choked, overflowing, or damaged drains, bad sewage smells, flooding.   \n",
    "            Output the category name. If none of the categories fits, or in doubt, return OTHER - PLEASE CHECK.  \n",
    "            '''\n",
    "\n",
    "            \n",
    "priority = '''Identify the priority to be given to the reported issues:\n",
    "            4-Low : the issue does not pose any problem with public safety and does not necessarily need to be handled urgently. \n",
    "            3-Medium : the issue does not cause any immediate danger, but it has significant and negative impact on the daily life of people in the neighborhood.\n",
    "            2-High : the issue needs to be resolved quickly because it can potentially cause dangerous situations or disruptions. \n",
    "            1-Very High : the issue needs to be handled as soon as possible, as it is a matter of public safety. \n",
    "            Return the priority level. If in doubt, return 3-Medium '''\n",
    "            \n",
    "        \n",
    "sentiment ='''Extract the sentiment of the post: \n",
    "            1. NEUTRAL: if the issue is reported politely\n",
    "            2. NEGATIVE: if the post shows irritation, impatience, annoyance\n",
    "            3. VERY NEGATIVE: the post expresses rage, hatred\n",
    "            '''\n",
    "\n",
    "address = ResponseSchema(name=\"address\",\n",
    "            description=\"Extract the address where the issue has been noticed. Return the street only and omit the town or country. For example: Oakwood Road.\")\n",
    "category = ResponseSchema(name=\"category\",\n",
    "            description=category)\n",
    "description = ResponseSchema(name=\"description\",\n",
    "            description=\"Summarize the issue that is being reported in not more that 300 characters and a neutral tone.\")\n",
    "location = ResponseSchema(name=\"location\",\n",
    "            description=\"Extract the coordinates where the issue has been notices. The format should be: (51.57470453612761,0.003792117010085437).\")\n",
    "priority = ResponseSchema(name=\"priority\",\n",
    "            description=priority)\n",
    "sentiment = ResponseSchema(name=\"sentiment\",\n",
    "            description=sentiment)\n",
    "summary = ResponseSchema(name=\"summary\",\n",
    "            description=\"Summarize the issue that is being reported in 40 characters and a neutral tone.\")\n",
    "        \n",
    "response_schemas = [\n",
    "            address,\n",
    "            category,\n",
    "            description,\n",
    "            location,\n",
    "            priority,\n",
    "            sentiment,\n",
    "            summary\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525e95f",
   "metadata": {},
   "source": [
    "Helper function to convert the social media post into string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1b0a3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_to_str(input_message):\n",
    "    #message = f\"redditPostId: {input_message[\"id\"]}, author: {input_message[\"author\"]}, title: {input_message[\"title\"]}, message: {input_message[\"longText\"]}, postingDate: {input_message[\"postingDate\"]}\"\n",
    "    message = \"redditPostId: \" + input_message[\"id\"]+\\\n",
    "            \", author: \"+input_message[\"author\"]+\", title: \"+input_message[\"title\"]+\\\n",
    "            \", message: \"+input_message[\"longText\"]+\", postingDate: \"+input_message[\"postingDate\"]\n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2735d34",
   "metadata": {},
   "source": [
    "Prepare the final prompt to extract the entities from the social medial post about public facility issue through citizen reporting app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50049c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"address\": string  // Extract the address where the issue has been noticed. Return the street only and omit the town or country. For example: Oakwood Road.\n",
      "\t\"category\": string  // Identify if the social media reports a situation related to one of the following categories: \n",
      "            1. PUBLIC CLEANLINESS: Dirty public areas, overflowing dustbins and littering. Bulky waste in common areas.  \n",
      "            2. ROADS & FOOTPATHS: Including covered linkways, signboards & streetlights. E.g. Pot holes, huge cracks, etc.\n",
      "            3. FACILITY & PARK MAINTENANCE: Fallen trees, overgrown grass, and maintenance of park lighting and facilities.\n",
      "            4. PESTS: Sighting of bees and hornets, potential mosquito breeding sites, and more.\n",
      "            5. DRAINS & SEWERS: Choked, overflowing, or damaged drains, bad sewage smells, flooding.   \n",
      "            Output the category name. If none of the categories fits, or in doubt, return OTHER - PLEASE CHECK.  \n",
      "            \n",
      "\t\"description\": string  // Summarize the issue that is being reported in not more that 300 characters and a neutral tone.\n",
      "\t\"location\": string  // Extract the coordinates where the issue has been notices. The format should be: (51.57470453612761,0.003792117010085437).\n",
      "\t\"priority\": string  // Identify the priority to be given to the reported issues:\n",
      "            4-Low : the issue does not pose any problem with public safety and does not necessarily need to be handled urgently. \n",
      "            3-Medium : the issue does not cause any immediate danger, but it has significant and negative impact on the daily life of people in the neighborhood.\n",
      "            2-High : the issue needs to be resolved quickly because it can potentially cause dangerous situations or disruptions. \n",
      "            1-Very High : the issue needs to be handled as soon as possible, as it is a matter of public safety. \n",
      "            Return the priority level. If in doubt, return 3-Medium \n",
      "\t\"sentiment\": string  // Extract the sentiment of the post: \n",
      "            1. NEUTRAL: if the issue is reported politely\n",
      "            2. NEGATIVE: if the post shows irritation, impatience, annoyance\n",
      "            3. VERY NEGATIVE: the post expresses rage, hatred\n",
      "            \n",
      "\t\"summary\": string  // Summarize the issue that is being reported in 40 characters and a neutral tone.\n",
      "}\n",
      "```\n",
      "[HumanMessage(content='Extract information from the following social media post: \\n            redditPostId: 198qqqm, author: jacobtan89, title: Dirty public area, message: The public area on Oakwood Road in Sagenai is in a disgraceful state with piles of rubbish and litter scattered everywhere. The author is frustrated with the local authorities for not maintaining cleanliness despite the taxes they pay. They hope for immediate action. #CleanUpYourAct #OakwoodRoadNightmare #DisgustingNeighborhood Coordinates:(51.57470453612761,0.003792117010085437), postingDate: 2024-01-17T07:13:48.000Z\\n            The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"address\": string  // Extract the address where the issue has been noticed. Return the street only and omit the town or country. For example: Oakwood Road.\\n\\t\"category\": string  // Identify if the social media reports a situation related to one of the following categories: \\n            1. PUBLIC CLEANLINESS: Dirty public areas, overflowing dustbins and littering. Bulky waste in common areas.  \\n            2. ROADS & FOOTPATHS: Including covered linkways, signboards & streetlights. E.g. Pot holes, huge cracks, etc.\\n            3. FACILITY & PARK MAINTENANCE: Fallen trees, overgrown grass, and maintenance of park lighting and facilities.\\n            4. PESTS: Sighting of bees and hornets, potential mosquito breeding sites, and more.\\n            5. DRAINS & SEWERS: Choked, overflowing, or damaged drains, bad sewage smells, flooding.   \\n            Output the category name. If none of the categories fits, or in doubt, return OTHER - PLEASE CHECK.  \\n            \\n\\t\"description\": string  // Summarize the issue that is being reported in not more that 300 characters and a neutral tone.\\n\\t\"location\": string  // Extract the coordinates where the issue has been notices. The format should be: (51.57470453612761,0.003792117010085437).\\n\\t\"priority\": string  // Identify the priority to be given to the reported issues:\\n            4-Low : the issue does not pose any problem with public safety and does not necessarily need to be handled urgently. \\n            3-Medium : the issue does not cause any immediate danger, but it has significant and negative impact on the daily life of people in the neighborhood.\\n            2-High : the issue needs to be resolved quickly because it can potentially cause dangerous situations or disruptions. \\n            1-Very High : the issue needs to be handled as soon as possible, as it is a matter of public safety. \\n            Return the priority level. If in doubt, return 3-Medium \\n\\t\"sentiment\": string  // Extract the sentiment of the post: \\n            1. NEUTRAL: if the issue is reported politely\\n            2. NEGATIVE: if the post shows irritation, impatience, annoyance\\n            3. VERY NEGATIVE: the post expresses rage, hatred\\n            \\n\\t\"summary\": string  // Summarize the issue that is being reported in 40 characters and a neutral tone.\\n}\\n```\\n            ')]\n"
     ]
    }
   ],
   "source": [
    "template_string = '''Extract information from the following social media post: \n",
    "            {post}\n",
    "            {format_instructions}\n",
    "            '''\n",
    "        \n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "        \n",
    "prompt_template = ChatPromptTemplate.from_template(template=template_string)\n",
    "input_message = {\n",
    "        \"id\": \"198qqqm\",\n",
    "        \"author\": \"jacobtan89\",\n",
    "        \"title\": \"Dirty public area\",\n",
    "        \"longText\": \"The public area on Oakwood Road in Sagenai is in a disgraceful state with piles of rubbish and litter scattered everywhere. The author is frustrated with the local authorities for not maintaining cleanliness despite the taxes they pay. They hope for immediate action. #CleanUpYourAct #OakwoodRoadNightmare #DisgustingNeighborhood Coordinates:(51.57470453612761,0.003792117010085437)\",\n",
    "        \"postingDate\": \"2024-01-17T07:13:48.000Z\"\n",
    "    }\n",
    "\n",
    "message = post_to_str(input_message)\n",
    "complete_prompt = prompt_template.format_messages(\n",
    "            post = message,\n",
    "            format_instructions = format_instructions\n",
    ")\n",
    "\n",
    "print(complete_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "03d14abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 3: Proxy with Langchain-like interface, together with Langchain components\n",
      " Here is the extracted information in the required markdown code snippet:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"address\": \"Oakwood Road\",\n",
      "    \"category\": \"PUBLIC CLEANLINESS\",\n",
      "    \"description\": \"Piles of rubbish and litter scattered everywhere, despite taxes being paid.\",\n",
      "    \"location\": \"(51.57470453612761,0.003792117010085437)\",\n",
      "    \"priority\": \"2-High\",\n",
      "    \"sentiment\": \"NEGATIVE\",\n",
      "    \"summary\": \"Dirty public area with litter\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    proxy_client=proxy_client,\n",
    "    deployment_id=deployment_id,\n",
    "    model_name=model,\n",
    "    temperature=0.5,\n",
    "    max_tokens=400,\n",
    "    # model_kwargs={\n",
    "    #     \"frequency_penalty\": -2, \"presence_penalty\": -1\n",
    "    # }\n",
    ")\n",
    "\n",
    "#llm_chain = complete_prompt | llm\n",
    "completion = llm.invoke(complete_prompt)\n",
    "\n",
    "print(\"Option 3: Proxy with Langchain-like interface, together with Langchain components\\n\",completion.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
